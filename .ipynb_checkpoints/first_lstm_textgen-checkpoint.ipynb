{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b185db0-4e25-4551-a59f-b5462669839f",
   "metadata": {},
   "source": [
    "## Basic LSTM text generator in PyTorch from guided tutorial at:\n",
    "https://machinelearningmastery.com/text-generation-with-lstm-in-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f1038-9634-479a-a504-e40d9e29a490",
   "metadata": {},
   "source": [
    "trained on: https://www.gutenberg.org/ebooks/11 with project gutenberg intro and outro removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8a57d-8e00-43df-9ae4-873f0799e0b0",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "plus get unique char set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b37c003b-ba38-42cb-9340-a86f4cc19623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  97920\n",
      "Total Vocab:  39\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"sonnets.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927dbcbb-5367-4c41-96c5-579230491829",
   "metadata": {},
   "source": [
    "### window over the data\n",
    "split data into X and Y, where X is array of arrays of **hyperparameter seq_length** characters and Y is the next chapter after this array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a898f0bd-a785-4c95-b78f-5396fc16e459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  97820\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae7d05-2c09-4419-aa25-70dcaf4c61a9",
   "metadata": {},
   "source": [
    "### Transform data to tensors\n",
    "and reshape X to be [number_of_sequences, length_of_sentence, num_of_features] where:  \n",
    "number_of_seq.. = amount of samples generated  \n",
    "length_of_sentence also known as **time steps** = seq_length  \n",
    "num_of_features = output len, 1 character or more  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09a076b-f7a9-4f7b-905e-e8c54b563c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97820, 100, 1]) torch.Size([97820])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\n",
    "X = X / float(n_vocab)\n",
    "y = torch.tensor(dataY)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65acc1e3-67ed-441a-9ea6-fd68794d1d3c",
   "metadata": {},
   "source": [
    "### Define simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21a4e4b-704e-4210-bfa3-b2ddb59f57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class CharModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(256, n_vocab)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        # take only the last output\n",
    "        x = x[:, -1, :]\n",
    "        # produce output\n",
    "        x = self.linear(self.dropout(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcea8db-c017-4253-b0dc-4a0c49653368",
   "metadata": {},
   "source": [
    "### Important: device setup for speed if GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "142d5399-5b41-4520-a0cd-0e299979454e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c02c8b-348a-465b-a51d-56d2db1b7fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ef628d9-9fc0-4169-a41f-b5e1a2c20517",
   "metadata": {},
   "source": [
    "### Train model\n",
    "* n_epochs\n",
    "* batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4cb3d8-9570-4082-8c17-083e7d4b2526",
   "metadata": {},
   "source": [
    "#### Batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65441f80-7f96-42ac-ada3-a17056a957ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 128\n",
    "model = CharModel()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    " \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "loader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aee2f9-beff-43d6-8e26-49da77e847dd",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "419479b9-acfc-447e-a85f-18d7f245bdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cross-entropy: 275209.8438\n",
      "Epoch 1: Cross-entropy: 260878.3594\n",
      "Epoch 2: Cross-entropy: 250564.0625\n",
      "Epoch 3: Cross-entropy: 246468.7969\n",
      "Epoch 4: Cross-entropy: 244189.2656\n",
      "Epoch 5: Cross-entropy: 240961.9844\n",
      "Epoch 6: Cross-entropy: 237438.6094\n",
      "Epoch 7: Cross-entropy: 236001.0156\n",
      "Epoch 8: Cross-entropy: 232041.2969\n",
      "Epoch 9: Cross-entropy: 229841.6250\n",
      "Epoch 10: Cross-entropy: 227710.4531\n",
      "Epoch 11: Cross-entropy: 224227.5312\n",
      "Epoch 12: Cross-entropy: 221384.6250\n",
      "Epoch 13: Cross-entropy: 218321.7656\n",
      "Epoch 14: Cross-entropy: 215453.1250\n",
      "Epoch 15: Cross-entropy: 212527.3750\n",
      "Epoch 16: Cross-entropy: 209258.3125\n",
      "Epoch 17: Cross-entropy: 206228.2500\n",
      "Epoch 18: Cross-entropy: 202893.5312\n",
      "Epoch 19: Cross-entropy: 200059.5312\n",
      "Epoch 20: Cross-entropy: 198487.7969\n",
      "Epoch 21: Cross-entropy: 194645.6094\n",
      "Epoch 22: Cross-entropy: 191912.0156\n",
      "Epoch 23: Cross-entropy: 188822.9844\n",
      "Epoch 24: Cross-entropy: 186370.4844\n",
      "Epoch 25: Cross-entropy: 183326.8906\n",
      "Epoch 26: Cross-entropy: 180954.7812\n",
      "Epoch 27: Cross-entropy: 179152.6562\n",
      "Epoch 28: Cross-entropy: 176461.4531\n",
      "Epoch 29: Cross-entropy: 173847.8281\n",
      "Epoch 30: Cross-entropy: 172702.9375\n",
      "Epoch 31: Cross-entropy: 170384.9844\n",
      "Epoch 32: Cross-entropy: 169853.2969\n",
      "Epoch 33: Cross-entropy: 165584.4844\n",
      "Epoch 34: Cross-entropy: 164203.1719\n",
      "Epoch 35: Cross-entropy: 192746.9688\n",
      "Epoch 36: Cross-entropy: 164905.3750\n",
      "Epoch 37: Cross-entropy: 160646.6719\n",
      "Epoch 38: Cross-entropy: 158421.4375\n",
      "Epoch 39: Cross-entropy: 159302.6562\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'char_to_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m             best_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m: Cross-entropy: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, loss))\n\u001b[1;32m---> 23\u001b[0m torch\u001b[38;5;241m.\u001b[39msave([best_model, \u001b[43mchar_to_dict\u001b[49m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle-char.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'char_to_dict' is not defined"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_loss = np.inf\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss += loss_fn(y_pred, y_batch)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = model.state_dict()\n",
    "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n",
    " \n",
    "torch.save([best_model, char_to_int], \"single-char.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c15eac8e-e7b3-493e-8cf9-2f9a0d4b159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save([best_model, char_to_int], \"single-char.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a1edec-16f0-41f8-b0fa-d3e73b40d911",
   "metadata": {},
   "source": [
    "### Generating text\n",
    "as the model is generating characters based on input characters of **seq_length**, to generate new data, we have to supply it with inputs of that format, eg. from other sonnets or (to test) from our data source / same author\n",
    "\n",
    "then, the model's output will be the predicted next character, which we can take and add at the end of our original input, and remove the input's first element, maintaining the input shape, (plus add the predicted char to our final output kept separately), then run through the model again and again for a specified number of times\n",
    "\n",
    "important note: LSTMs dont just return the end character, but also it's internal state history, therefore, from the output we have to make a decision whether to only take the last output or try to include the previous states and argmax them or otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb395b-be25-41a9-9094-fbf4b87efadd",
   "metadata": {},
   "source": [
    "##### getting a random sentence from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce53557d-f366-4fe2-9d57-f543266d7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "start = np.random.randint(0, len(raw_text)-seq_length)\n",
    "prompt = raw_text[start:start+seq_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fadecc-c733-47ff-b99d-fe7b36723b9f",
   "metadata": {},
   "source": [
    "or you could try passing in a left-padded array of seq_len to start without any data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ebb0e-55e2-4c87-82ed-076f96be95a3",
   "metadata": {},
   "source": [
    "##### Transform the input into proper format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10b26a4d-2bb6-4cc3-8f20-a433a670044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [char_to_int[c] for c in prompt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f780109-5fb0-4797-8db7-44a558bff9ad",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e88f7b07-f159-49b3-b09a-cce45e99dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cc58313-b659-478c-9705-576564267cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"ll grind\n",
      " on newer proof, to try an older friend,\n",
      " a god in love, to whom i am confin'd.\n",
      "   then giv\"\n",
      "e, not thoue, thy love' thy love shal wort.\n",
      "\n",
      " lxxiii\n",
      "\n",
      " then mo the sorengs so the wirld toeel oowe\n",
      " and tour the baau and thene ooo derprede,\n",
      " and toen the tare to thee in thei whll shee,\n",
      " and there ooo mo bear eear hev soen lo toene,\n",
      " and thene ae thine, and the tire that toee  and to the romi of toet wher iedves siahe,\n",
      " the caiest derter dan hacr hotm tiee,    thes thou dester te the tire that iove thee soote,\n",
      "   then thou aettres thite whth thee, and then to toowe.\n",
      "\n",
      " xxii\n",
      "\n",
      " nh the world sfat soici siee hor lo thee,\n",
      " and tour in then if touth io touth to gesr,\n",
      " and there ooo mo horerg certer derterss,\n",
      " which that this soogse that thich i foor thee,\n",
      " and theu foo mo and thin iet soeel coowent,\n",
      " woth she loe to thire whuh thut lose so lroe,\n",
      " thou has she lerercics of the wirl of lroes,\n",
      " whine ie thou singss thatl to thls drty'st,\n",
      " and thereiore shat thich whth soulh to thee    aad see eect sorle  aad siei sorl deenee\n",
      "   then thou dester teee, which thou dester seie.\n",
      "\n",
      " cxxiii\n",
      "\n",
      " the love\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print('Prompt: \"%s\"' % prompt)\n",
    "with torch.no_grad():\n",
    "    for i in range(output_length):\n",
    "        # format input array of int into PyTorch tensor\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
    "        x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        # generate logits as output from the model\n",
    "        prediction = model(x)\n",
    "        # convert logits into one character\n",
    "        index = int(prediction.argmax())\n",
    "        result = int_to_char[index]\n",
    "        print(result, end=\"\")\n",
    "        # append the new character into the prompt for the next iteration\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:]\n",
    "print()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734107c5-8414-4856-b0f6-6f27d5a3bc1d",
   "metadata": {},
   "source": [
    "##### Extra: loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf63ec5f-ccd5-40e3-9d42-4603c4c16a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model, char_to_int = torch.load(\"single-char.pth\")\n",
    "# n_vocab = len(char_to_int)\n",
    "# int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
    " \n",
    "# # reload the model\n",
    "# class CharModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "#         self.linear = nn.Linear(256, n_vocab)\n",
    "#     def forward(self, x):\n",
    "#         x, _ = self.lstm(x)\n",
    "#         # take only the last output\n",
    "#         x = x[:, -1, :]\n",
    "#         # produce output\n",
    "#         x = self.linear(self.dropout(x))\n",
    "#         return x\n",
    "# model = CharModel()\n",
    "# model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934efb4a-1937-4c8c-9c25-aea1d877ff6c",
   "metadata": {},
   "source": [
    "### Bigger model, more interesting data\n",
    "more layers (1 -> 2)  \n",
    "bigger hidden state (256 -> 500)  \n",
    "plus a dropout between the lstm layers  \n",
    "more epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b5414-8c9c-4cce-8bf2-991c565b38e7",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54c1e59b-4654-492b-9020-e5c4bfcc655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  13326\n",
      "Total Vocab:  46\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"doom_lyrics_10.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1170441-b3a8-4d12-9d51-dde1a4eba990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  13226\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "664442de-25b7-41b6-a99a-66f4f9c8af42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13226, 100, 1]) torch.Size([13226])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\n",
    "X = X / float(n_vocab)\n",
    "y = torch.tensor(dataY)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d82404-6b6b-41a0-943a-c4d3a1b5b846",
   "metadata": {},
   "source": [
    "#### Model def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e067c807-02f3-4348-942d-0fc283589d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class CharModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=500, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(500, n_vocab)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        # take only the last output\n",
    "        x = x[:, -1, :]\n",
    "        # produce output\n",
    "        x = self.linear(self.dropout(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f06145-926c-4aaa-a31e-dbaa3839b329",
   "metadata": {},
   "source": [
    "#### Hyperparams and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2d7c578-905e-4652-95d0-c34fad343ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 128\n",
    "model = CharModel()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    " \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "loader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293bb9c-b9e8-4828-a099-fe6f8db251aa",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f91105d-3ab1-42c5-81d7-186854340f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cross-entropy: 40262.7773\n",
      "Epoch 1: Cross-entropy: 39590.3281\n",
      "Epoch 2: Cross-entropy: 38013.3828\n",
      "Epoch 3: Cross-entropy: 36848.4453\n",
      "Epoch 4: Cross-entropy: 36428.7383\n",
      "Epoch 5: Cross-entropy: 35954.5273\n",
      "Epoch 6: Cross-entropy: 36024.7188\n",
      "Epoch 7: Cross-entropy: 34768.0039\n",
      "Epoch 8: Cross-entropy: 34082.7227\n",
      "Epoch 9: Cross-entropy: 33125.3086\n",
      "Epoch 10: Cross-entropy: 32378.8516\n",
      "Epoch 11: Cross-entropy: 31193.6641\n",
      "Epoch 12: Cross-entropy: 29897.7891\n",
      "Epoch 13: Cross-entropy: 27899.1758\n",
      "Epoch 14: Cross-entropy: 25787.2090\n",
      "Epoch 15: Cross-entropy: 23417.8770\n",
      "Epoch 16: Cross-entropy: 20840.6992\n",
      "Epoch 17: Cross-entropy: 18289.4531\n",
      "Epoch 18: Cross-entropy: 16143.0479\n",
      "Epoch 19: Cross-entropy: 13825.5479\n",
      "Epoch 20: Cross-entropy: 11637.7578\n",
      "Epoch 21: Cross-entropy: 9663.5566\n",
      "Epoch 22: Cross-entropy: 8113.5322\n",
      "Epoch 23: Cross-entropy: 6587.5669\n",
      "Epoch 24: Cross-entropy: 5276.6514\n",
      "Epoch 25: Cross-entropy: 4259.2539\n",
      "Epoch 26: Cross-entropy: 3258.2354\n",
      "Epoch 27: Cross-entropy: 2554.8892\n",
      "Epoch 28: Cross-entropy: 2025.4397\n",
      "Epoch 29: Cross-entropy: 1570.6832\n",
      "Epoch 30: Cross-entropy: 1212.2889\n",
      "Epoch 31: Cross-entropy: 1020.9920\n",
      "Epoch 32: Cross-entropy: 760.6448\n",
      "Epoch 33: Cross-entropy: 651.3705\n",
      "Epoch 34: Cross-entropy: 492.9142\n",
      "Epoch 35: Cross-entropy: 425.2150\n",
      "Epoch 36: Cross-entropy: 347.4948\n",
      "Epoch 37: Cross-entropy: 315.0630\n",
      "Epoch 38: Cross-entropy: 263.4556\n",
      "Epoch 39: Cross-entropy: 239.4729\n",
      "Epoch 40: Cross-entropy: 239.3790\n",
      "Epoch 41: Cross-entropy: 242.9935\n",
      "Epoch 42: Cross-entropy: 202.8164\n",
      "Epoch 43: Cross-entropy: 162.8539\n",
      "Epoch 44: Cross-entropy: 148.6962\n",
      "Epoch 45: Cross-entropy: 203.4994\n",
      "Epoch 46: Cross-entropy: 174.9052\n",
      "Epoch 47: Cross-entropy: 161.6290\n",
      "Epoch 48: Cross-entropy: 168.8103\n",
      "Epoch 49: Cross-entropy: 285.9521\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_loss = np.inf\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss += loss_fn(y_pred, y_batch)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = model.state_dict()\n",
    "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n",
    " \n",
    "torch.save([best_model, char_to_int], \"doom_bigger.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a25b0-e6d9-47bc-b4a4-36660cb5a167",
   "metadata": {},
   "source": [
    "#### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ffc669b-2f80-4738-860d-29490b2ff792",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "start = np.random.randint(0, len(raw_text)-seq_length)\n",
    "prompt = raw_text[start:start+seq_length]\n",
    "\n",
    "pattern = [char_to_int[c] for c in prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fad2f888-2b19-4b9b-9bef-ba3a97abfb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_length = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88fd00a6-c59e-41e7-854a-99d2756f06c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"he smack just missed her\n",
      "there go a list of politics like henry kissinger\n",
      "99% of rap's just a friend\"\n",
      "ly listener\n",
      "i'm like, \"these dudes must have some screws loose to hate y'all\"\n",
      "or a couple of ounces short of deuce-deuce or 8 ball\n",
      "y'all know it's time for the end when the day come\n",
      "buy a album, get rudely insulted over fake drums\n",
      "same cds you get fo\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print('Prompt: \"%s\"' % prompt)\n",
    "with torch.no_grad():\n",
    "    for i in range(output_length):\n",
    "        # format input array of int into PyTorch tensor\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
    "        x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        # generate logits as output from the model\n",
    "        prediction = model(x)\n",
    "        # convert logits into one character\n",
    "        index = int(prediction.argmax())\n",
    "        result = int_to_char[index]\n",
    "        print(result, end=\"\")\n",
    "        # append the new character into the prompt for the next iteration\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:]\n",
    "print()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927d541-a38c-4d66-a116-5b4ae6b7955d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
