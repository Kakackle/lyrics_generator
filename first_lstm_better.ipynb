{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46714f5a-d924-43b2-a928-5339e7afbb0a",
   "metadata": {},
   "source": [
    "## Basic LSTM rewrite for rease of further experiments\n",
    "\n",
    "still loosely based on:  \n",
    "https://machinelearningmastery.com/text-generation-with-lstm-in-pytorch/\n",
    "\n",
    "but also:  \n",
    "https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/10_time_series_forecasting_in_tensorflow.ipynb\n",
    "\n",
    "and some:  \n",
    "https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/08_introduction_to_nlp_in_tensorflow.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c720c-5453-4ee8-9ee7-9b3c8b648167",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "519f08eb-913e-4e6e-8ace-105e5233dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a51792-76ad-4c29-9a15-059e383eb41c",
   "metadata": {},
   "source": [
    "### Function to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24119514-e06e-4026-9f9e-05df5d28647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_data(filename):\n",
    "    raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "    raw_text = raw_text.lower()\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61b4ee-6f63-4cac-9420-d4aa9eb9066f",
   "metadata": {},
   "source": [
    "### Function to Map Chars to Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f46de28-3df8-4155-a4f1-16d0e1f23f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_chars_to_int(raw_text):\n",
    "    # create mapping of unique chars to integers\n",
    "    chars = sorted(list(set(raw_text)))\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "    int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
    "\n",
    "    n_chars = len(raw_text)\n",
    "    vocab_length = len(chars)\n",
    "    print(\"Total Characters: \", n_chars)\n",
    "    print(\"Vocab len: \", vocab_length)\n",
    "    \n",
    "    return chars, char_to_int, int_to_char, vocab_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087cb93-5173-4b1e-8524-1d5288cb2119",
   "metadata": {},
   "source": [
    "### Function to window over data (currently unused)\n",
    "to create training data and labels  \n",
    "with specified sequence (input) window length and horizon (output) len\n",
    "\n",
    "only works with numbers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71aa40e6-c4b6-4e31-8bb4-f6c081528035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to label windowed data\n",
    "def get_labelled_windows(x, horizon=1):\n",
    "    \"\"\"\n",
    "    Creates labels for windowed dataset.\n",
    "    \n",
    "    E.g. if horizon=1 (default)\n",
    "    Input: [1, 2, 3, 4, 5, 6] -> Output: ([1, 2, 3, 4, 5], [6])\n",
    "    \"\"\"\n",
    "    return x[:, :-horizon], x[:, -horizon:]\n",
    "\n",
    "\n",
    "def make_windows(x, window_size, horizon = 1):\n",
    "    \"\"\"\n",
    "    Turns a 1D array into a 2D array of sequential windows of window_size.\n",
    "    \"\"\"\n",
    "    # 1. Create a window of specific window_size (add the horizon on the end for later labelling)\n",
    "    window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
    "    # print(f\"Window step:\\n {window_step}\")\n",
    "    \n",
    "    # 2. Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)\n",
    "    window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size\n",
    "    # print(f\"Window indexes:\\n {window_indexes[:3], window_indexes[-3:], window_indexes.shape}\")\n",
    "    \n",
    "    # 3. Index on the target array (time series) with 2D array of multiple window steps\n",
    "    windowed_array = x[window_indexes]\n",
    "    \n",
    "    # 4. Get the labelled windows\n",
    "    windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
    "    \n",
    "    return windows, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3aaed-9836-41a5-bb88-ec9c4227310b",
   "metadata": {},
   "source": [
    "### Function to window over chars and convert chars to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43d5f079-c740-4cfc-a81a-e07086dc647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chars_to_numbers_X_Y(raw_data, chars, char_to_int, window_size, horizon):\n",
    "def chars_to_numbers_X_Y_1(raw_data, chars, char_to_int, window_size):\n",
    "    n_chars = len(raw_data)\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, n_chars - window_size, 1):\n",
    "        seq_in = raw_text[i:i + window_size]\n",
    "        # seq_out = raw_text[i + window_size: i + window_size + horizon]\n",
    "        seq_out = raw_text[i+window_size]\n",
    "        dataX.append([char_to_int[char] for char in seq_in])\n",
    "        # dataY.append([char_to_int[char] for char in seq_out])\n",
    "        dataY.append(char_to_int[seq_out])\n",
    "    return dataX, dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c856239-e85b-44a9-999a-d5ac6975b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chars, char_to_int, int_to_char, vocab_length = map_chars_to_int(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "754f421c-0af4-416a-8451-494b949b9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataX, dataY = chars_to_numbers_X_Y(raw_text, chars, char_to_int, seq_length, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5979be53-f247-4e9b-b53a-185a03f477de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_X = len(dataX)\n",
    "# len_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863bb3b-d370-4d31-b603-c358df8c326b",
   "metadata": {},
   "source": [
    "### Function to reshape data for training and turn into tensors\n",
    "reshape X to be [number_of_sequences, length_of_sentence, num_of_features] where:  \n",
    "number_of_seq.. = amount of samples generated  \n",
    "length_of_sentence also known as time steps = seq_length  \n",
    "num_of_features = output len, 1 character or more  \n",
    "\n",
    "plus normalization of data by vocab_length - total number of distinct characters - **necesarry or not?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9273509b-b8eb-4c15-bcb6-31b005260598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_train_data(x, y, window_size, horizon, device, vocab_length):\n",
    "    n_seq = len(x)\n",
    "    X = torch.tensor(x, dtype=torch.float32).reshape(n_seq, window_size, horizon).to(device)\n",
    "    X = X / float(vocab_length)\n",
    "    y = torch.tensor(y).to(device)\n",
    "    y = y.squeeze()\n",
    "    print(X.shape, y.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c85311ac-00c6-4b22-9ade-af062f8d8ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = reshape_train_data(dataX, dataY, seq_length, horizon, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9937d-496e-46fd-a5ca-c7e31d42c060",
   "metadata": {},
   "source": [
    "### Function to define LSTM-only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f664feaa-9386-46fb-b4e5-82a859b75f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(vocab_length, input_size=1, hidden_size=256, num_layers=2, dropout_perc=0.2, batch_first=True):\n",
    "    \"\"\"\n",
    "    we pass in vocab_length, to define linear after LSTM output range, because that's the number of possible characters\n",
    "    the network output could be\n",
    "    \"\"\"\n",
    "    class CharModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.lstm = nn.LSTM(input_size=input_size,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_layers=num_layers,\n",
    "                                batch_first=batch_first,\n",
    "                                dropout=dropout_perc)\n",
    "            self.dropout = nn.Dropout(dropout_perc)\n",
    "            self.linear = nn.Linear(hidden_size, vocab_length)\n",
    "        def forward(self, x):\n",
    "            x, _ = self.lstm(x)\n",
    "            # take only the last output\n",
    "            x = x[:, -1, :]\n",
    "            # produce output\n",
    "            x = self.linear(self.dropout(x))\n",
    "            return x\n",
    "    model = CharModel()\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59eca5-c49e-4cc6-a6d1-599d864c881c",
   "metadata": {},
   "source": [
    "### Define traning optimalizer, loss function, data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a413a534-6ac5-4b8b-b6dc-7c1ce5774635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adam_loss_loader(X, y, model, batch_size, lr):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)\n",
    "    return optimizer, loss_fn, loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a947df4-760c-418c-88d5-6027db2ce09b",
   "metadata": {},
   "source": [
    "### Function to train model\n",
    "and save the best model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b682aec9-1fa4-468d-96e7-8e46a3cb5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loss_fn, loader, epochs, device):\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch.to(device))\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        loss = 0\n",
    "        with torch.inference_mode():\n",
    "            for X_batch, y_batch in loader:\n",
    "                y_pred = model(X_batch.to(device))\n",
    "                loss += loss_fn(y_pred, y_batch)\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_model = model.state_dict()\n",
    "            print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45021b27-0fcf-40cd-899d-6cfd949fd980",
   "metadata": {},
   "source": [
    "### Functions to save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d96e7a81-9dcf-43f1-ad94-45c4b32d9655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_char_model_with_mappings(best_model, file_name, char_to_int, int_to_char):\n",
    "    \"\"\"\n",
    "    char_to_int / char mappign saved along with model\n",
    "    \"\"\"\n",
    "    torch.save([best_model, char_to_int, int_to_char], f\"{file_name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3420a7-4b7c-4191-8761-091274217cb0",
   "metadata": {},
   "source": [
    "### Function to evaluate the model / make example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc4095db-d90f-436a-bc22-6cd0204d99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_raw_line(raw_text, seq_length, char_to_int):\n",
    "    start = np.random.randint(0, len(raw_text)-seq_length)\n",
    "    prompt = raw_text[start:start+seq_length]\n",
    "    print(f\"Chosen prompt: {prompt}\")\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f32fcd68-0d38-4353-9db0-656b636c9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_char_model(prompt, model, char_to_int, int_to_char, vocab_length, output_length = 250):\n",
    "    \"\"\"\n",
    "    vocab_length for normalization\n",
    "    \"\"\"\n",
    "    pattern = [char_to_int[c] for c in prompt]\n",
    "    print('Prompt: \"%s\"' % prompt)\n",
    "    with torch.inference_mode():\n",
    "        for i in range(output_length):\n",
    "            # format input array of int into PyTorch tensor\n",
    "            x = np.reshape(pattern, (1, len(pattern), 1)) / float(vocab_length)\n",
    "            x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "            # generate logits as output from the model\n",
    "            prediction = model(x)\n",
    "            # convert logits into one character\n",
    "            index = int(prediction.argmax())\n",
    "            result = int_to_char[index]\n",
    "            print(result, end=\"\")\n",
    "            # append the new character into the prompt for the next iteration\n",
    "            pattern.append(index)\n",
    "            pattern = pattern[1:]\n",
    "    print()\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6106d479-fa32-4497-88e7-79bef3c0c321",
   "metadata": {},
   "source": [
    "### function to load model from state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11298972-d2b0-4a03-8f8c-a84be16ae26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_model(filename):\n",
    "#     best_model, char_to_int, int_to_char = torch.load(filename)\n",
    "# # reload the model\n",
    "# class CharModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "#         self.linear = nn.Linear(256, n_vocab)\n",
    "#     def forward(self, x):\n",
    "#         x, _ = self.lstm(x)\n",
    "#         # take only the last output\n",
    "#         x = x[:, -1, :]\n",
    "#         # produce output\n",
    "#         x = self.linear(self.dropout(x))\n",
    "#         return x\n",
    "# model = CharModel()\n",
    "# model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4980f5a-eb2c-4c31-972b-255b11cc3e27",
   "metadata": {},
   "source": [
    "## Basic function usage (for char-by-char LSTM and raw data start point):\n",
    "1. raw_text = load_text_data(filename)\n",
    "2. chars, char_to_int, int_to_char, vocab_length = map_chars_to_int(raw_text)\n",
    "3. dataX, dataY = chars_to_numbers_X_Y(raw_data, chars, char_to_int, window_size, horizon) ... or chars_to_number_X_Y_1 for horizon of 1 (default for now)\n",
    "4. X, y = reshape_train_data(x, y, window_size, horizon, device, vocab_length)\n",
    "5. model = create_LSTM_model(vocab_length, input_size=1, hidden_size=256, num_layers=2, dropout_perc=0.2, batch_first=True)\n",
    "6. create_adam_loss_loader(X, y, model, batch_size, lr) -> optimizer, loss_fn, loader\n",
    "7. best_model = train_model(model, optimizer, loss_fn, loader, epochs)\n",
    "8. save_char_model_with_mappings(best_model, file_name, char_to_int, int_to_char)\n",
    "9. prompt = get_random_raw_line(raw_text, seq_length)\n",
    "10. eval_char_model(prompt, model, char_to_int, int_to_char, vocab_length, output_length = 250)\n",
    "\n",
    "where:\n",
    "* filename - path to file with raw text data\n",
    "* window_size = seq_length - length of input the model will be trained to predict based on\n",
    "* horizon = input_size - the length of its output (one character, many characters, words etc) \n",
    "* device - \"cuda\" or \"cpu\"\n",
    "* optimizer = Adam etc\n",
    "* lossfn = crossentrophy etc\n",
    "* char_to_int, int_to_char - mapping functions\n",
    "* output_length - predicted eval text length\n",
    "* epochs - tranign epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ff0f2-87f0-4dbc-98e3-26c51bee0ae7",
   "metadata": {},
   "source": [
    "### Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cdbbcb9-03c0-4ad6-816c-0c03b5561aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "# filename = \"mfdoom_100.txt\"\n",
    "filename = \"mfdoom_10.txt\"\n",
    "filename_no_ext = \"mfdoom_10\"\n",
    "\n",
    "# input window\n",
    "window_size = 100 #seq_length\n",
    "# output window / predicted\n",
    "horizon = 1\n",
    "\n",
    "# --------- training -----------\n",
    "# torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#\n",
    "n_epochs = 50\n",
    "batch_size = 256\n",
    "\n",
    "###\n",
    "hidden_size = 500\n",
    "num_layers = 3\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "lr_str = str(lr).replace('.', '_')\n",
    "\n",
    "model_type = \"char_LSTM\"\n",
    "\n",
    "experiment_name = f'{model_type}_{filename_no_ext}_window_{window_size}_horizon_{horizon}_epochs_{n_epochs}_hidden_{hidden_size}_layers_{num_layers}_lr_{lr_str}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "12e739e0-cef0-441f-9d63-e2552ba265fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = load_text_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4502f882-4ef9-4c17-abb5-c098c294e99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  95040\n",
      "Vocab len:  64\n"
     ]
    }
   ],
   "source": [
    "chars, char_to_int, int_to_char, vocab_length = map_chars_to_int(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c185bd7-8950-424a-9960-951312028509",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX, dataY = chars_to_numbers_X_Y_1(raw_text, chars, char_to_int, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4310c482-86ac-4175-a395-ff80ff442501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34, 31, 1, 49, 34]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataX[:5]\n",
    "len(dataX)\n",
    "dataY[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ba70648-57e2-468d-a968-14138c811b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([94940, 100, 1]) torch.Size([94940])\n"
     ]
    }
   ],
   "source": [
    "X, y = reshape_train_data(dataX, dataY, window_size, horizon, device, vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9df605b3-e57d-4777-86ce-ea75470fafc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([34, 31], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e7460e03-fd84-4ce8-999d-61b05c82f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_doom_big = create_LSTM_model(vocab_length, hidden_size=256, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db00b597-962c-4e60-bd55-f028a21a2a43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimizer, loss_fn, loader \u001b[38;5;241m=\u001b[39m create_adam_loss_loader(\u001b[43mX\u001b[49m, y, model_doom_big, batch_size, lr)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer, loss_fn, loader = create_adam_loss_loader(X, y, model_doom_big, batch_size, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7181dc09-9bb2-4de8-bf4f-229941b3b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6ff854db-d2f6-45ac-b776-4dcd2859423a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cross-entropy: 289788.6250\n",
      "Epoch 1: Cross-entropy: 270306.1562\n",
      "Epoch 2: Cross-entropy: 264447.8125\n",
      "Epoch 3: Cross-entropy: 259005.6562\n",
      "Epoch 4: Cross-entropy: 253187.5781\n",
      "Epoch 5: Cross-entropy: 248495.3594\n",
      "Epoch 6: Cross-entropy: 244034.0625\n",
      "Epoch 7: Cross-entropy: 240350.9062\n",
      "Epoch 8: Cross-entropy: 236372.0938\n",
      "Epoch 9: Cross-entropy: 233145.3906\n",
      "Epoch 10: Cross-entropy: 230923.9375\n",
      "Epoch 11: Cross-entropy: 231813.6875\n",
      "Epoch 12: Cross-entropy: 224254.0156\n",
      "Epoch 13: Cross-entropy: 220783.9375\n",
      "Epoch 14: Cross-entropy: 220087.6406\n",
      "Epoch 15: Cross-entropy: 216040.7656\n",
      "Epoch 16: Cross-entropy: 213813.8906\n",
      "Epoch 17: Cross-entropy: 213026.5156\n",
      "Epoch 18: Cross-entropy: 208641.2500\n",
      "Epoch 19: Cross-entropy: 206389.4531\n",
      "Epoch 20: Cross-entropy: 203122.1875\n",
      "Epoch 21: Cross-entropy: 200168.4688\n",
      "Epoch 22: Cross-entropy: 202001.5312\n",
      "Epoch 23: Cross-entropy: 195821.3125\n",
      "Epoch 24: Cross-entropy: 193132.1875\n",
      "Epoch 25: Cross-entropy: 190988.9375\n",
      "Epoch 26: Cross-entropy: 188532.8594\n",
      "Epoch 27: Cross-entropy: 186704.2188\n",
      "Epoch 28: Cross-entropy: 184819.2656\n",
      "Epoch 29: Cross-entropy: 182133.5938\n",
      "Epoch 30: Cross-entropy: 180399.5625\n",
      "Epoch 31: Cross-entropy: 178851.1562\n",
      "Epoch 32: Cross-entropy: 176015.5781\n",
      "Epoch 33: Cross-entropy: 174624.0156\n",
      "Epoch 34: Cross-entropy: 171507.2812\n",
      "Epoch 35: Cross-entropy: 170497.2344\n",
      "Epoch 36: Cross-entropy: 167397.8594\n",
      "Epoch 37: Cross-entropy: 165630.4219\n",
      "Epoch 38: Cross-entropy: 163849.2031\n",
      "Epoch 39: Cross-entropy: 161742.4688\n",
      "Epoch 40: Cross-entropy: 161650.0781\n",
      "Epoch 41: Cross-entropy: 158744.5312\n",
      "Epoch 42: Cross-entropy: 157340.5000\n",
      "Epoch 43: Cross-entropy: 154695.1719\n",
      "Epoch 44: Cross-entropy: 153126.0000\n",
      "Epoch 45: Cross-entropy: 151477.7188\n",
      "Epoch 46: Cross-entropy: 150355.6562\n",
      "Epoch 47: Cross-entropy: 151339.2969\n",
      "Epoch 48: Cross-entropy: 146699.9219\n",
      "Epoch 49: Cross-entropy: 146237.5938\n"
     ]
    }
   ],
   "source": [
    "best_model_doom_big = train_model(model_doom_big, optimizer, loss_fn, loader, experiment_name, n_epochs, char_to_int, int_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d9729b3-d7a5-416f-8517-413474a5f9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen prompt: effects\n",
      "before you press charges use your noodle\n",
      "so what when he grab the mic he crush your cute cut\n"
     ]
    }
   ],
   "source": [
    "prompt = get_random_raw_line(raw_text, window_size, char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d6a07a98-25c8-4f7c-acb8-ed19a077dca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"effects\n",
      "before you press charges use your noodle\n",
      "so what when he grab the mic he crush your cute cut\"\n",
      " the word ‘ou might also like\n",
      "iow mand the same saidns bead beat\n",
      "\n",
      "ye can sepping all thet\n",
      "in the forn like she word ‘ou might also like\n",
      "io so she coon with a bount of the sooe whth the broker then i meed to het she said the wante hear to the case to \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "eval_model(prompt = prompt,\n",
    "           model = model_doom_big,\n",
    "           char_to_int = char_to_int,\n",
    "           int_to_char = int_to_char,\n",
    "           vocab_length = vocab_length,\n",
    "           output_length = 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5945dff-39ee-4d4f-9fe5-506866d8ad8e",
   "metadata": {},
   "source": [
    "### TODOs:\n",
    "1. function chars_to_numbers_X_Y for now only works with horizon = 1, fix that - generally - generating multiple characters / words with RNNs\n",
    "2. slowa - jesli tak, to inne kroki zamiast char itd, wiec wciaz troche chuj a nie uniwersalne)\n",
    "3. TorchText?\n",
    "4. inni artysci (na koniec?)\n",
    "5. inne architektury (powinno zamieniac wtedy tylko krok z modelem)\n",
    "6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "219242fe-c5d7-440a-abcb-1bcfd01efbea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave([\u001b[43mbest_model\u001b[49m, char_to_int, int_to_char], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "# torch.save([best_model, char_to_int, int_to_char], f\"{experiment_name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffef6b5-df12-43fd-9b12-644b09fed1cc",
   "metadata": {},
   "source": [
    "## Word tokenizing and embeddings with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee62e4df-948f-4d36-81ba-cee35003a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1087f6a-4627-4792-9417-f1ef5a35e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "import en_core_web_trf\n",
    "nlp = en_core_web_trf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28558ec1-839b-40ec-abcf-ecd1e821e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(\"This is a sentence.\")\n",
    "# print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6204765-8622-46f9-8813-f6a982771f19",
   "metadata": {},
   "source": [
    "### Function to load the data from file into a worldlist with possible extra preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e9298f-2ca4-477f-b636-8c95809e00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordlist(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "        # create wordlist\n",
    "        doc = nlp(data)\n",
    "        wordlist = []\n",
    "        for word in doc:\n",
    "            # + extra preprocessing, like excluding certain characters\n",
    "            if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "                wordlist.append(word.text.lower())\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b3de8-d419-421f-b81b-af9828d536d9",
   "metadata": {},
   "source": [
    "#### Function to create word counts, a vocabulary and word mapping to indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e3a594-f841-4ca2-8053-1f2d15162d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_counts(wordlist):\n",
    "    # count the number of words\n",
    "    word_counts = collections.Counter(wordlist)\n",
    "    \n",
    "    # Mapping from index to word : that's the vocabulary\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()] # if most_common parameter omitted, returns all words\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "    # Mapping from word to index\n",
    "    vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    words = [x[0] for x in word_counts.most_common()]\n",
    "\n",
    "    #size of the vocabulary\n",
    "    vocab_size = len(words)\n",
    "    print(\"vocab size: \", vocab_size)\n",
    "\n",
    "    return vocab, words, vocab_size\n",
    "    #save the words and vocabulary\n",
    "    # with open(os.path.join(vocab_file), 'wb') as f:\n",
    "    #     cPickle.dump((words, vocab, vocabulary_inv), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed5c8f4-9809-45ab-a0e7-32ec5474feef",
   "metadata": {},
   "source": [
    "### Split word data into X (window) and Y (horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c72995cc-b6a7-4290-a631-fd4d6bba36e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_windows(wordlist, window_size):\n",
    "    #create sequences\n",
    "    sequences = []\n",
    "    next_words = []\n",
    "    for i in range(0, len(wordlist) - window_size, 1):\n",
    "        sequences.append(wordlist[i: i + window_size])\n",
    "        next_words.append(wordlist[i + window_size])\n",
    "    \n",
    "    print('len X:', len(sequences))\n",
    "    print('len y:', len(next_words))\n",
    "    return sequences, next_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af967953-fad1-4222-91aa-0db9fa521df6",
   "metadata": {},
   "source": [
    "### Transform windowed data to index form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f531640f-7eff-480a-a5bc-7a84c6230712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_X_Y(sequences, next_words, window_size, vocab, vocab_size):  \n",
    "    X = np.zeros((len(sequences), window_size, vocab_size), dtype=np.float32)\n",
    "    y = np.zeros((len(sequences), vocab_size), dtype=np.bool_)\n",
    "    for i, sentence in enumerate(sequences):\n",
    "        for t, word in enumerate(sentence):\n",
    "            X[i, t, vocab[word]] = 1\n",
    "        y[i, vocab[next_words[i]]] = 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee5057-9905-4be9-a85f-113791c66614",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. create_wordlist(filepath) -> wordlist\n",
    "2. create_word_counts(wordlist) -> vocab, words, vocab_size\n",
    "3. create_word_windows(wordlist, window_size) -> sequences, next_words\n",
    "4. create_word_X_Y(sequences, next_words, window_size, vocab, vocab_size) -> X, y\n",
    "5. device setup\n",
    "6. model = create_LSTM_model(vocab_length, input_size=1, hidden_size=256, num_layers=2, dropout_perc=0.2, batch_first=True)\n",
    "7. create_adam_loss_loader(X, y, model, batch_size, lr) -> optimizer, loss_fn, loader\r",
    "8. \n",
    "best_model = train_model(model, optimizer, loss_fn, loader, epochs\n",
    "9. save_word_model\n",
    "10. prompt = get_random_raw_line(raw_text, seq_length)\n",
    "11. eval_word_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6209c2-6eab-4f99-a324-9e9471343c79",
   "metadata": {},
   "source": [
    "#### Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dee6d2a-104d-4257-b0da-3dbb07ed8d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "# filename = \"mfdoom_100.txt\"\n",
    "filename = \"mfdoom_50.txt\"\n",
    "filename_no_ext = \"mfdoom_50\"\n",
    "\n",
    "# input window\n",
    "window_size = 10 #seq_length\n",
    "# output window / predicted\n",
    "horizon = 1\n",
    "\n",
    "# --------- training -----------\n",
    "# torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#\n",
    "n_epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "###\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "lr_str = str(lr).replace('.', '_')\n",
    "\n",
    "model_type = \"word_LSTM\"\n",
    "\n",
    "experiment_name = f'{model_type}_{filename_no_ext}_window_{window_size}_horizon_{horizon}_epochs_{n_epochs}_hidden_{hidden_size}_layers_{num_layers}_lr_{lr_str}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb083403-4355-46e7-84b1-b06c0831e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = create_wordlist(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d37a7bbf-b58e-4171-a1e5-c746860e2d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  4262\n"
     ]
    }
   ],
   "source": [
    "vocab, words, vocab_size = create_word_counts(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e484d221-1576-4d33-970c-63928c525723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len X: 21467\n",
      "len y: 21467\n"
     ]
    }
   ],
   "source": [
    "sequences, next_words = create_word_windows(wordlist, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3be31bb8-cf23-4561-b3c2-41599934a86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = create_word_X_Y(sequences, next_words, window_size, vocab, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca1ab7e1-05ca-474b-9a24-2703e30749c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_Y_to_numbers_reshape(sequences, next_words, window_size, vocab, vocab_size, device):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i, seq in enumerate(sequences):\n",
    "        dataX.append([vocab[char] for char in seq])\n",
    "        dataY.append(vocab[next_words[i]])\n",
    "    n_seq = len(sequences)\n",
    "    X = torch.tensor(dataX, dtype=torch.float32).reshape(n_seq, window_size, 1).to(device)\n",
    "    X = X / float(vocab_size)\n",
    "    y = torch.tensor(dataY).to(device)\n",
    "    y = y.squeeze()\n",
    "    print(X.shape, y.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99a14154-8a3a-41f7-a761-ebc3da45f4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21467, 10, 1]) torch.Size([21467])\n"
     ]
    }
   ],
   "source": [
    "X, y = X_Y_to_numbers_reshape(sequences, next_words, window_size, vocab, vocab_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "13c7ef50-a6d8-4eac-b2ff-3c09e74f65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.tensor(X).to(device)\n",
    "# y = torch.tensor(y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "88485d02-b64c-4232-9904-f90d7221ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doom_small_model = create_LSTM_model(vocab_length = vocab_size,\n",
    "                                          input_size = 1,\n",
    "                                          hidden_size = hidden_size,\n",
    "                                          num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "be649f93-cd63-43ab-aa92-57ceac512d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, loss_fn, loader = create_adam_loss_loader(X, y, word_doom_small_model, batch_size, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f2ed813e-a04c-4999-95a5-b541b696a3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cross-entropy: 134334.3281\n",
      "Epoch 1: Cross-entropy: 133306.8906\n",
      "Epoch 2: Cross-entropy: 132716.8125\n",
      "Epoch 3: Cross-entropy: 132247.0312\n",
      "Epoch 4: Cross-entropy: 131984.1719\n",
      "Epoch 5: Cross-entropy: 131511.8906\n",
      "Epoch 6: Cross-entropy: 131423.7188\n",
      "Epoch 7: Cross-entropy: 130830.5000\n",
      "Epoch 8: Cross-entropy: 130729.2500\n",
      "Epoch 9: Cross-entropy: 130268.3125\n",
      "Epoch 10: Cross-entropy: 130054.8672\n",
      "Epoch 11: Cross-entropy: 130292.2891\n",
      "Epoch 12: Cross-entropy: 129344.4062\n",
      "Epoch 13: Cross-entropy: 129343.7812\n",
      "Epoch 14: Cross-entropy: 129209.1016\n",
      "Epoch 15: Cross-entropy: 128723.7812\n",
      "Epoch 16: Cross-entropy: 128777.2344\n",
      "Epoch 17: Cross-entropy: 128699.2344\n",
      "Epoch 18: Cross-entropy: 128166.9375\n",
      "Epoch 19: Cross-entropy: 127921.9141\n",
      "Epoch 20: Cross-entropy: 127977.9219\n",
      "Epoch 21: Cross-entropy: 127375.3125\n",
      "Epoch 22: Cross-entropy: 127022.9922\n",
      "Epoch 23: Cross-entropy: 127292.4688\n",
      "Epoch 24: Cross-entropy: 126703.6641\n",
      "Epoch 25: Cross-entropy: 126816.2656\n",
      "Epoch 26: Cross-entropy: 126589.6328\n",
      "Epoch 27: Cross-entropy: 126106.3125\n",
      "Epoch 28: Cross-entropy: 125939.2891\n",
      "Epoch 29: Cross-entropy: 125795.3125\n",
      "Epoch 30: Cross-entropy: 125507.8594\n",
      "Epoch 31: Cross-entropy: 125029.7500\n",
      "Epoch 32: Cross-entropy: 124871.6484\n",
      "Epoch 33: Cross-entropy: 124653.2500\n",
      "Epoch 34: Cross-entropy: 124224.5938\n",
      "Epoch 35: Cross-entropy: 124336.7031\n",
      "Epoch 36: Cross-entropy: 123811.0547\n",
      "Epoch 37: Cross-entropy: 123662.1250\n",
      "Epoch 38: Cross-entropy: 123795.3516\n",
      "Epoch 39: Cross-entropy: 123420.5156\n",
      "Epoch 40: Cross-entropy: 122625.3203\n",
      "Epoch 41: Cross-entropy: 123393.7031\n",
      "Epoch 42: Cross-entropy: 123750.8203\n",
      "Epoch 43: Cross-entropy: 122698.9141\n",
      "Epoch 44: Cross-entropy: 122234.7812\n",
      "Epoch 45: Cross-entropy: 121580.4844\n",
      "Epoch 46: Cross-entropy: 121740.4375\n",
      "Epoch 47: Cross-entropy: 121295.5234\n",
      "Epoch 48: Cross-entropy: 121459.8594\n",
      "Epoch 49: Cross-entropy: 121392.0391\n",
      "Epoch 50: Cross-entropy: 121353.8359\n",
      "Epoch 51: Cross-entropy: 120702.1797\n",
      "Epoch 52: Cross-entropy: 120198.1641\n",
      "Epoch 53: Cross-entropy: 120143.2500\n",
      "Epoch 54: Cross-entropy: 119971.4609\n",
      "Epoch 55: Cross-entropy: 119951.7344\n",
      "Epoch 56: Cross-entropy: 120481.0859\n",
      "Epoch 57: Cross-entropy: 119487.4062\n",
      "Epoch 58: Cross-entropy: 119783.2891\n",
      "Epoch 59: Cross-entropy: 119262.2812\n",
      "Epoch 60: Cross-entropy: 119359.2578\n",
      "Epoch 61: Cross-entropy: 119340.8750\n",
      "Epoch 62: Cross-entropy: 118718.5078\n",
      "Epoch 63: Cross-entropy: 118583.2500\n",
      "Epoch 64: Cross-entropy: 118106.5547\n",
      "Epoch 65: Cross-entropy: 119018.2969\n",
      "Epoch 66: Cross-entropy: 118431.8281\n",
      "Epoch 67: Cross-entropy: 118402.4766\n",
      "Epoch 68: Cross-entropy: 117687.5156\n",
      "Epoch 69: Cross-entropy: 117650.0625\n",
      "Epoch 70: Cross-entropy: 118258.1250\n",
      "Epoch 71: Cross-entropy: 119818.5000\n",
      "Epoch 72: Cross-entropy: 117478.9922\n",
      "Epoch 73: Cross-entropy: 117666.9531\n",
      "Epoch 74: Cross-entropy: 117832.7500\n",
      "Epoch 75: Cross-entropy: 117224.8828\n",
      "Epoch 76: Cross-entropy: 116896.8047\n",
      "Epoch 77: Cross-entropy: 116506.2031\n",
      "Epoch 78: Cross-entropy: 116282.4219\n",
      "Epoch 79: Cross-entropy: 117528.6016\n",
      "Epoch 80: Cross-entropy: 116131.8750\n",
      "Epoch 81: Cross-entropy: 117103.0625\n",
      "Epoch 82: Cross-entropy: 116044.6719\n",
      "Epoch 83: Cross-entropy: 118375.5938\n",
      "Epoch 84: Cross-entropy: 116155.2578\n",
      "Epoch 85: Cross-entropy: 116077.1797\n",
      "Epoch 86: Cross-entropy: 115576.2188\n",
      "Epoch 87: Cross-entropy: 115742.2969\n",
      "Epoch 88: Cross-entropy: 117093.7812\n",
      "Epoch 89: Cross-entropy: 115304.8828\n",
      "Epoch 90: Cross-entropy: 114815.7812\n",
      "Epoch 91: Cross-entropy: 115229.8047\n",
      "Epoch 92: Cross-entropy: 115363.5547\n",
      "Epoch 93: Cross-entropy: 114416.8672\n",
      "Epoch 94: Cross-entropy: 114650.7578\n",
      "Epoch 95: Cross-entropy: 114628.6328\n",
      "Epoch 96: Cross-entropy: 116028.4844\n",
      "Epoch 97: Cross-entropy: 114305.6094\n",
      "Epoch 98: Cross-entropy: 114800.7969\n",
      "Epoch 99: Cross-entropy: 116555.5938\n"
     ]
    }
   ],
   "source": [
    "best_model = train_model(word_doom_small_model, optimizer, loss_fn, loader, n_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b0080763-7e63-47bb-b306-acf1a788df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e47ec09f-9521-487e-8f46-ebe4b804be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "be276501-c7f3-4e3e-8968-c5c7dce7032c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4d35bfa0-6e73-49cc-b592-6f4a38e6bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca79a9de-8fcd-41ed-a0a8-cf408e3ceeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_raw_words(wordlist, seq_length):\n",
    "    start = np.random.randint(0, len(wordlist)-seq_length)\n",
    "    prompt = wordlist[start:start+seq_length]\n",
    "    print(f\"Chosen prompt: {prompt}\")\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0854c9d-ae5a-4beb-8560-2590dd30fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = collections.Counter(wordlist)    \n",
    "# Mapping from index to word : that's the vocabulary\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()] # if most_common parameter omitted, returns all words\n",
    "vocabulary_inv = list(sorted(vocabulary_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7cd82d6-46c0-490f-bb86-416de1ad86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_word_model(prompt, model, vocab, vocab_length, output_length = 100):\n",
    "    \"\"\"\n",
    "    vocab_length for normalization\n",
    "    \"\"\"\n",
    "    result_total = []\n",
    "    pattern = [vocab[c] for c in prompt]\n",
    "    print('Prompt: \"%s\"' % prompt)\n",
    "    with torch.inference_mode():\n",
    "        for i in range(output_length):\n",
    "            # format input array of int into PyTorch tensor\n",
    "            x = np.reshape(pattern, (1, len(pattern), 1)) / float(vocab_length)\n",
    "            x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "            # generate logits as output from the model\n",
    "            prediction = model(x)\n",
    "            # convert logits into one character\n",
    "            index = int(prediction.argmax())\n",
    "            result = vocabulary_inv[index]\n",
    "            result_total.append(result)\n",
    "            # print(result, end=\"\")\n",
    "            # append the new character into the prompt for the next iteration\n",
    "            pattern.append(index)\n",
    "            pattern = pattern[1:]\n",
    "    print(' '.join(result_total))\n",
    "    print(\"Done.\")\n",
    "    return result_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "debf5273-8daf-45ba-acc8-0287324a8418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen prompt: [',', 'peeps', ',', 'brothers', ',', 'sisters', ',', 'duns', ',', 'dunnies']\n"
     ]
    }
   ],
   "source": [
    "prompt = get_random_raw_words(wordlist, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "286a2b08-da10-49ea-974a-49b0829ee8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"[',', 'peeps', ',', 'brothers', ',', 'sisters', ',', 'duns', ',', 'dunnies']\"\n",
      ", , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "result = eval_word_model(prompt, word_doom_small_model, vocab, vocab_size, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b01a64dc-58b1-4bd4-b859-7c3ba1224717",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save([best_model, vocab, vocabulary_inv], f\"{experiment_name}.pth\")\n",
    "# torch.save([best_model, vocab, vocabulary_inv, hidden_size, num_layers, dropout_perc, window_size], f\"{experiment_name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bc844-90ef-4e60-b77a-e411e260299b",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c50eaa8d-b71d-4988-b1e2-b04291b9e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"word_LSTM_mfdoom_10_window_10_horizon_1_epochs_50_hidden_256_layers_2_lr_0_01.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03677b76-7829-4982-82c6-82156d32da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, vocab, vocabulary_inv = torch.load(model_path)\n",
    "# best_model, vocab, vocabulary_inv, hidden_size, num_layers, dropout_perc, window_size = torch.load(model_path)\n",
    "vocab_length = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68c9b63e-9703-4c9d-9879-a5e841e56709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = CharModel()\n",
    "# model.load_state_dict(best_model)\n",
    "loaded_model = create_LSTM_model(vocab_length, input_size=1,\n",
    "                  hidden_size=hidden_size,\n",
    "                  num_layers=num_layers,\n",
    "                  dropout_perc=0.2,\n",
    "                  batch_first=True)\n",
    "loaded_model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d162432-8a7b-40a0-aeff-6a54fda8be5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen prompt: ['(', 'misc', '.', 'voice', '):', 'food', ',', 'we', 'need', 'food']\n"
     ]
    }
   ],
   "source": [
    "prompt = get_random_raw_words(wordlist, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13d9f8f6-ef51-4bba-a794-8c902764b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"[',', 'lock', '-', 'down', ',', 'wet', 'dreams', 'of', 'fox', \"'\"]\"\n",
      "brown on doomsday ! , ever since the womb ‘ til i 'm back where my brother went , that 's what my tomb will say right above my government ; dumile with know drew in her only \" that , that got and now 's loco 'm well open these they shut ( voice last time i split the wishbone a man first throwing mcs pox you it it her face , lay beer , do n't , \" rock lives 's poor , un nigga ? we shall now vote doom , the or . i down n't\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "result = eval_word_model(prompt, loaded_model, vocab, vocab_size, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15a8bf7f-cda0-4180-841b-57021d5f78b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(', 'misc', '.', 'voice', '):']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d4b9750-a16b-4fe0-88ae-f7696a6bce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [vocab[c] for c in prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb9115b1-3222-4839-be4e-f79e0bead65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 2337, 21, 4035, 17]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3555375-7c53-497c-80c2-a5b4f7ef196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "003409ff-2625-422c-a12e-5ec86ac16051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float32')\n",
    "    preds = np.log(np.abs(preds)) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "db7206ff-f6ee-4dc9-b253-c22380690d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816\n",
      "2698\n",
      "1314\n",
      "3624\n",
      "1536\n",
      "3044\n",
      "623\n",
      "2344\n",
      "2181\n",
      "2388\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    for i in range(output_len):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1)) / float(vocab_length)\n",
    "        x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        # print(x)\n",
    "        prediction = loaded_model(x)\n",
    "        # argm = prediction.argmax()\n",
    "        sampled = sample(prediction.cpu().squeeze())\n",
    "        # index = int(sampled)\n",
    "        # argm = argm.cpu()\n",
    "        # print(prediction)\n",
    "        print(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0e3c6a5f-fbc8-4843-89ba-b943c8e9d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_word_model_with_sampling(prompt, model, vocab, vocab_length, temperature = 0.8, output_length = 100):\n",
    "    \"\"\"\n",
    "    vocab_length for normalization\n",
    "    \"\"\"\n",
    "    result_total = []\n",
    "    pattern = [vocab[c] for c in prompt]\n",
    "    print('Prompt: \"%s\"' % prompt)\n",
    "    with torch.inference_mode():\n",
    "        for i in range(output_length):\n",
    "            # format input array of int into PyTorch tensor\n",
    "            x = np.reshape(pattern, (1, len(pattern), 1)) / float(vocab_length)\n",
    "            x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "            # generate logits as output from the model\n",
    "            prediction = model(x)\n",
    "            index = sample(prediction.cpu().squeeze(), temperature=temperature)\n",
    "            # convert logits into one character\n",
    "            # index = int(prediction.argmax())\n",
    "            result = vocabulary_inv[index]\n",
    "            result_total.append(result)\n",
    "            # print(result, end=\"\")\n",
    "            # append the new character into the prompt for the next iteration\n",
    "            pattern.append(index)\n",
    "            pattern = pattern[1:]\n",
    "    print(' '.join(result_total))\n",
    "    print(\"Done.\")\n",
    "    return result_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "33a62e88-6478-4c7a-a337-66734e2ec339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"['(', 'misc', '.', 'voice', '):', 'food', ',', 'we', 'need', 'food']\"\n",
      "always surprise bounce feeling bugged powerful queen extorter jar dot crabs awfully seductive blast autograph release vic eagle stink phases chalk mention whities rookies jason licker jam sneak beer arteries south % gloves triple smoking trading skill hella permission conducting treat planets lunch blazing rudely retarded trickles icicles rhyming address berries fingernail frontin poultry chance flesh suggest bike rapping whiner machine cheez states intercoms doze ayo pain incredible games briz joking actions gravy thigh tears : insulted noose receive dictionary bootlegger speed throws mescalines benjamin forget quit blooper jiminy thuggers haha slicker thief temazepam sweet kissed duck negro toke trust\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['always',\n",
       " 'surprise',\n",
       " 'bounce',\n",
       " 'feeling',\n",
       " 'bugged',\n",
       " 'powerful',\n",
       " 'queen',\n",
       " 'extorter',\n",
       " 'jar',\n",
       " 'dot',\n",
       " 'crabs',\n",
       " 'awfully',\n",
       " 'seductive',\n",
       " 'blast',\n",
       " 'autograph',\n",
       " 'release',\n",
       " 'vic',\n",
       " 'eagle',\n",
       " 'stink',\n",
       " 'phases',\n",
       " 'chalk',\n",
       " 'mention',\n",
       " 'whities',\n",
       " 'rookies',\n",
       " 'jason',\n",
       " 'licker',\n",
       " 'jam',\n",
       " 'sneak',\n",
       " 'beer',\n",
       " 'arteries',\n",
       " 'south',\n",
       " '%',\n",
       " 'gloves',\n",
       " 'triple',\n",
       " 'smoking',\n",
       " 'trading',\n",
       " 'skill',\n",
       " 'hella',\n",
       " 'permission',\n",
       " 'conducting',\n",
       " 'treat',\n",
       " 'planets',\n",
       " 'lunch',\n",
       " 'blazing',\n",
       " 'rudely',\n",
       " 'retarded',\n",
       " 'trickles',\n",
       " 'icicles',\n",
       " 'rhyming',\n",
       " 'address',\n",
       " 'berries',\n",
       " 'fingernail',\n",
       " 'frontin',\n",
       " 'poultry',\n",
       " 'chance',\n",
       " 'flesh',\n",
       " 'suggest',\n",
       " 'bike',\n",
       " 'rapping',\n",
       " 'whiner',\n",
       " 'machine',\n",
       " 'cheez',\n",
       " 'states',\n",
       " 'intercoms',\n",
       " 'doze',\n",
       " 'ayo',\n",
       " 'pain',\n",
       " 'incredible',\n",
       " 'games',\n",
       " 'briz',\n",
       " 'joking',\n",
       " 'actions',\n",
       " 'gravy',\n",
       " 'thigh',\n",
       " 'tears',\n",
       " ':',\n",
       " 'insulted',\n",
       " 'noose',\n",
       " 'receive',\n",
       " 'dictionary',\n",
       " 'bootlegger',\n",
       " 'speed',\n",
       " 'throws',\n",
       " 'mescalines',\n",
       " 'benjamin',\n",
       " 'forget',\n",
       " 'quit',\n",
       " 'blooper',\n",
       " 'jiminy',\n",
       " 'thuggers',\n",
       " 'haha',\n",
       " 'slicker',\n",
       " 'thief',\n",
       " 'temazepam',\n",
       " 'sweet',\n",
       " 'kissed',\n",
       " 'duck',\n",
       " 'negro',\n",
       " 'toke',\n",
       " 'trust']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_word_model_with_sampling(prompt, loaded_model, vocab, vocab_length, 0.8, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
